#Parallel-Computing 
# CUDA Execution Model
- The CUDA/OpenCL/SYCL Execution Model is:
	- An "integrated **host** + **device** app C program".
		- The device is also known as a "kernel".
		- The pattern is run sequential code, then run parallel code, wait for the parallel code to finish, then get back to your sequential code, repeat.
	- A CUDA kernel is executed by a grid (array) of threads.
		- All threads in a grid run the same kernel code.
		- "Single Program, Multiple Data" (SPMD)
		- Each thread has an index that it uses to compute memory addresses and make control decisions.
		- The hierarchy is as follows: kernel -> grid -> block -> thread
			- So to access an index in the device, it is `i = blockIdx.x * blockDim.x + threadIdx.x;`
	- We can furthermore divide the thread array into multiple blocks.
		- Threads within a block cooperate via a shared memory, atomic operations, and barrier synchronization.
		- Threads in different blocks cooperate less.
	- We define the dimensions of grids and blocks.
- See Vector Addition example.
- As for function declarations:
	- ![[Pasted image 20230117125747.png]]
- ![[Pasted image 20230117130712.png]]
- `cudaMalloc`, `cudaFree`, and `cudaMemcpy`