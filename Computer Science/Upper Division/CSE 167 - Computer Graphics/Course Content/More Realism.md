## Environment Mapping
- Centered around the question as to what color/texture to assign the fragment.
	- Based on how light is reflected.
- We map using panoramas to convert resulting directions to coordinates where a texture is stored.
	- Spheres might have some distortion, so...
	- Cube maps are preferred.
		- We prepare an image that shows all the distant surroundings in a cube-net.
- In the first pass, the environment map is generated by rendering 6 images for the cube map, each stored as textrues for the environment map.
- The second pass simply creates the final image.

## Bump Mapping
- Bumps add an organic and thus a more realistic feel to rendered objects.
	- However, we don't want to generate a ton more triangles (subdividing the surface) than we have already to make these bumps.
	- If we try to shortcut using texture mapping with a picture of the real object, the shading of the bumps won't match the lighting in our scene.
- Our solution is to use an image that specifies the bumps as **normals**.
	- Note that the Phong shading from before interpolates normals across the object surfaces in order to compute the light for each fragment.
	- Bump mapping will store normals in a bumpmap texture instead.
		- We can think of the bump map as just a height field of sorts.
- With all that said, the silhouette of the object itself will be missing the bumps.

## Shadow Mapping
- Note how shadows give additional visual cues... ![[image - shadows create depth.png]]
- ...and realism. Consider "self-shadowing", for instance, hair shading itself when it overlaps itself.
- We call the **umbra** the fully shadowed region and the **penumbra** the partially shadowed region. The **occluder** is what blocks the light in the first place.
	- The umbra is a "hard" shadow, which there are tricks to compute for.
- We call that trick "shadow mapping".
	- The main idea is as such:
		- We know that a point on the scene is lit up if the light source has a clear path to that point.
		- Think of it instead as "visibility" from the light source by placing a camera at the light source position.
![[image - rethinking light.png]]
- So with two passes:
	- Render the scene by putting the camera at the light and store the "depth map" (the z-values) in a texture buffer.
	- Then, render the scene from the actual camera, where at each pixel we'll compare the distance to the light with the depth map from the first pass.
		- Fundamentally, if the depth value for the eye is "much bigger" than the depth value for the light, then you can assume it's in a shadow.
		- We need a *tolerance* value to determine what exactly is "much bigger" to avoid some silly bugs.