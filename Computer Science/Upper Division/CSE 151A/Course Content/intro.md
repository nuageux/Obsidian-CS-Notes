past 30 years
- machine learning is more effective thanks to
	- computer speed and memory
	- way more data, since everything is recorded
	- evolution of the discipline
		- connections to related fields
		- theoretical foundation
- probability, linalg, optimization

nearest neighbor classification
- classic "identify handwritten numbers" problem
	- "given an image of a handwritten digit, say which digit it is"
	- post office was interested in this!
- machine learning approach
	- for a classification problem, specify it through examples.
- for n training images, get x^1, x^2, ..., x^n and corresponding labels y^1, y^2, ..., y^n. labels are the "correct answers" for each x.
	- now for the new image, just find within the n images the image that looks the closest to this new image and label it as such.
	- ..."closeness"? measuring the "distance" between images
	- use euclidean distance in 2d. pythagorean theorem.
	- the error rate on training points is zero, obviously
		- instead use test error. turns out, it's 3.09%
	- we can try better distance functions or k-nearest neighbor
		- find the k closest images, then take the most common label from amongem
		- but the best choice of k is 3! for this problem. in settings where there isn't a clear answer, k-nn is good.
			- what do we do without the test set?
			- CROSS VALIDATION!
				- the problem with the holdout set is that it weakens our prediction.
				- validation set should be a certain absolute size.
			- "leave one out": just don't check the matching think in the training set!
- data augmentation
- or tangent distance (1.1%) which covers small translations and rotations... or shape context (.63%) which examines a broader family of natural deformations
- m-fold cross-validation
	- 